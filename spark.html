
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Usage with Apache Spark on YARN &#8212; conda-pack 0.2.0+2.g60a8143 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="CLI Docs" href="cli.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="usage-with-apache-spark-on-yarn">
<h1>Usage with Apache Spark on YARN<a class="headerlink" href="#usage-with-apache-spark-on-yarn" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">conda-pack</span></code> can be used to distribute conda environments to be used with
<a class="reference external" href="http://spark.apache.org/">Apache Spark</a> jobs when <a class="reference external" href="http://spark.apache.org/docs/latest/running-on-yarn.html">deploying on Apache YARN</a>. By bundling your
environment for use with Spark, you can make use of all the libraries provided
by <code class="docutils literal notranslate"><span class="pre">conda</span></code>, and ensure that they’re consistently provided on every node. This
makes use of <a class="reference external" href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html">YARN’s</a>
resource localization by distributing environments as archives, which are then
automatically unarchived on every node. In this case either the <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> or
<code class="docutils literal notranslate"><span class="pre">zip</span></code> formats must be used.</p>
<div class="section" id="python-example">
<h2>Python Example<a class="headerlink" href="#python-example" title="Permalink to this headline">¶</a></h2>
<p>Create an environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda create -y -n example <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.5 numpy pandas scikit-learn
</pre></div>
</div>
<p>Activate the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda activate example   <span class="c1"># Older conda versions use `source activate` instead</span>
</pre></div>
</div>
<p>Package the environment into a <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> archive:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda pack -o environment.tar.gz
Collecting packages...
Packing environment at <span class="s1">&#39;/Users/jcrist/anaconda/envs/example&#39;</span> to <span class="s1">&#39;environment.tar.gz&#39;</span>
<span class="o">[</span><span class="c1">########################################] | 100% Completed | 23.2s</span>
</pre></div>
</div>
<p>Write a PySpark script, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># script.py</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span>
<span class="n">conf</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s1">&#39;spark-yarn&#39;</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">some_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Packages are imported and available from your bundled environment.</span>
    <span class="kn">import</span> <span class="nn">sklearn</span>
    <span class="kn">import</span> <span class="nn">pandas</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

    <span class="c1"># Use the libraries to do work</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span>

<span class="n">rdd</span> <span class="o">=</span> <span class="p">(</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
         <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">some_function</span><span class="p">)</span>
         <span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span>
</pre></div>
</div>
<p>Submit the job to Spark using <code class="docutils literal notranslate"><span class="pre">spark-submit</span></code>. In YARN cluster mode:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ <span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>./environment/bin/python <span class="se">\</span>
spark-submit <span class="se">\</span>
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON<span class="o">=</span>./environment/bin/python <span class="se">\</span>
--master yarn <span class="se">\</span>
--deploy-mode cluster <span class="se">\</span>
--archives environment.tar.gz#environment <span class="se">\</span>
script.py
</pre></div>
</div>
<p>Or in YARN client mode:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span><span class="sb">`</span>which python<span class="sb">`</span> <span class="se">\</span>
<span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>./environment/bin/python <span class="se">\</span>
spark-submit <span class="se">\</span>
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON<span class="o">=</span>./environment/bin/python <span class="se">\</span>
--master yarn <span class="se">\</span>
--deploy-mode client <span class="se">\</span>
--archives environment.tar.gz#environment <span class="se">\</span>
script.py
</pre></div>
</div>
<p>You can also start a PySpark interactive session using the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ <span class="nv">PYSPARK_DRIVER_PYTHON</span><span class="o">=</span><span class="sb">`</span>which python<span class="sb">`</span> <span class="se">\</span>
<span class="nv">PYSPARK_PYTHON</span><span class="o">=</span>./environment/bin/python <span class="se">\</span>
pyspark <span class="se">\</span>
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON<span class="o">=</span>./environment/bin/python <span class="se">\</span>
--master yarn <span class="se">\</span>
--deploy-mode client <span class="se">\</span>
--archives environment.tar.gz#environment
</pre></div>
</div>
</div>
<div class="section" id="r-example">
<h2>R Example<a class="headerlink" href="#r-example" title="Permalink to this headline">¶</a></h2>
<p>Conda also supports R environments. Here we’ll demonstrate creating and
packaging an environment for use with <a class="reference external" href="http://spark.rstudio.com/">Sparklyr</a>.
Note that similar techniques also work with <a class="reference external" href="https://spark.apache.org/docs/latest/sparkr.html">SparkR</a>.</p>
<p>First, create an environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda create -y -n example r-sparklyr
</pre></div>
</div>
<p>Activate the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda activate example   <span class="c1"># Older conda versions use `source activate` instead</span>
</pre></div>
</div>
<p>Package the environment into a <code class="docutils literal notranslate"><span class="pre">tar.gz</span></code> archive. Note the addition of the
<code class="docutils literal notranslate"><span class="pre">-d</span> <span class="pre">./environment</span></code> flag. This tells <code class="docutils literal notranslate"><span class="pre">conda-pack</span></code> to rewrite the any
prefixes to the path <code class="docutils literal notranslate"><span class="pre">./environment</span></code> (the relative path to the environment
from the working directory on the YARN workers) before packaging. This is
required for R, as the R executables have absolute paths hardcoded in them
(whereas Python does not).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ conda pack -o environment.tar.gz -d ./environment
Collecting packages...
Packing environment at <span class="s1">&#39;/Users/jcrist/anaconda/envs/example&#39;</span> to <span class="s1">&#39;environment.tar.gz&#39;</span>
<span class="o">[</span><span class="c1">########################################] | 100% Completed | 21.8s</span>
</pre></div>
</div>
<p>Write an R script, for example:</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="kn">library</span><span class="p">(</span>sparklyr<span class="p">)</span>

<span class="c1"># Create a spark configuration</span>
config <span class="o">&lt;-</span> spark_config<span class="p">()</span>

<span class="c1"># Specify that the packaged environment should be distributed</span>
<span class="c1"># and unpacked to the directory &quot;environment&quot;</span>
config<span class="o">$</span>spark.yarn.dist.archives <span class="o">&lt;-</span> <span class="s">&quot;environment.tar.gz#environment&quot;</span>

<span class="c1"># Specify the R command to use, as well as various R locations on the workers</span>
config<span class="o">$</span>spark.r.command <span class="o">&lt;-</span> <span class="s">&quot;./environment/bin/Rscript&quot;</span>
config<span class="o">$</span>sparklyr.apply.env.R_HOME <span class="o">&lt;-</span> <span class="s">&quot;./environment/lib/R&quot;</span>
config<span class="o">$</span>sparklyr.apply.env.RHOME <span class="o">&lt;-</span> <span class="s">&quot;./environment&quot;</span>
config<span class="o">$</span>sparklyr.apply.env.R_SHARE_DIR <span class="o">&lt;-</span> <span class="s">&quot;./environment/lib/R/share&quot;</span>
config<span class="o">$</span>sparklyr.apply.env.R_INCLUDE_DIR <span class="o">&lt;-</span> <span class="s">&quot;./environment/lib/R/include&quot;</span>

<span class="c1"># Create a spark context.</span>
<span class="c1"># You can also specify `master = &quot;yarn-cluster&quot;` for cluster mode.</span>
sc <span class="o">&lt;-</span> spark_connect<span class="p">(</span>master <span class="o">=</span> <span class="s">&quot;yarn-client&quot;</span><span class="p">,</span> config <span class="o">=</span> config<span class="p">)</span>

<span class="c1"># Use a user defined function, which requires a working R environment on</span>
<span class="c1"># every worker node. Since all R packages already exist on every node, we</span>
<span class="c1"># pass in ``packages = FALSE`` to avoid redistributing them.</span>
sdf_copy_to<span class="p">(</span>sc<span class="p">,</span> iris<span class="p">)</span> <span class="o">%&gt;%</span>
    spark_apply<span class="p">(</span><span class="kr">function</span><span class="p">(</span>e<span class="p">)</span> broom<span class="o">::</span>tidy<span class="p">(</span>lm<span class="p">(</span>Petal_Length <span class="o">~</span> Petal_Width<span class="p">,</span> e<span class="p">)),</span>
                packages <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span>
</pre></div>
</div>
<p>Run the script.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ Rscript script.R
<span class="c1"># Source:   table&lt;sparklyr_tmp_12de794b4e2a&gt; [?? x 5]</span>
<span class="c1"># Database: spark_connection</span>
  Sepal_Length Sepal_Width Petal_Length Petal_Width  Species
  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;
<span class="m">1</span> <span class="o">(</span>Intercept<span class="o">)</span>         <span class="m">1</span>.08       <span class="m">0</span>.0730        <span class="m">14</span>.8 <span class="m">4</span>.04e-31
<span class="m">2</span> Petal_Width         <span class="m">2</span>.23       <span class="m">0</span>.0514        <span class="m">43</span>.4 <span class="m">4</span>.68e-86
</pre></div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">conda-pack</a></h1>



<p class="blurb">A tool for packaging and distributing conda environments.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=conda&repo=conda-pack&type=watch&count=False&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





    

<p>
<a href="https://travis-ci.org/conda/conda-pack">
    <img
        alt="https://secure.travis-ci.org/conda/conda-pack.svg?branch=master"
        src="https://secure.travis-ci.org/conda/conda-pack.svg?branch=master"
    />
</a>
</p>


<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api.html">API Docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI Docs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Usage with Apache Spark on YARN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#python-example">Python Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#r-example">R Example</a></li>
</ul>
</li>
</ul>

<h3>Need help?</h3>

<p>
  Open an issue in the <a href="https://github.com/conda/conda-pack/issues">issue tracker</a>.
</p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Jim Crist.
      
      |
      <a href="_sources/spark.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>